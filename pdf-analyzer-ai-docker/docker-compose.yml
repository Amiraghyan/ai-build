version: "3.9"

services:
  # ────────────── Frontend Vite/NGINX ──────────────
  frontend:
    build:
      context: .
      args:
        # Utilisé par le Dockerfile du front → VITE_API_BASE_URL
        VITE_API_BASE_URL: http://backend:8000
    ports:
      - "8080:80"               # http://localhost:8080
    depends_on:
      backend:
        condition: service_healthy
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  # ────────────── Backend FastAPI ──────────────
  backend:
    build:
      context: ./backend        # Dockerfile dans backend/
    environment:
      - PYTHONUNBUFFERED=1
      - OLLAMA_HOST=http://ollama:11434
      - OLLAMA_MODEL=llama3.2:3b   # modèle déjà « pull » côté Ollama
      - OLLAMA_MAX_CHARS=15000
    expose:
      - "8000"                  # interne seulement (front → backend)
    depends_on:
      ollama:
        condition: service_started
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:8000/"]
      interval: 30s
      timeout: 5s
      retries: 3

  # ────────────── Daemon Ollama ──────────────
  ollama:
    image: ollama/ollama:latest
    volumes:
      - ollama-data:/root/.ollama   # persiste les modèles (.ollama)
    restart: unless-stopped
    # Décommente si tu veux accéder à l’API Ollama hors Docker :
    # ports:
    #   - "11434:11434"

volumes:
  ollama-data:
